---
# Documentation: https://wowchemy.com/docs/managing-content/

title: 'Exploring the Landscape of Recommender Systems Evaluation: Practices and Perspectives'
subtitle: ''
summary: ''
authors:
- Christine Bauer
- Eva Zangerle
- Alan Said
tags:
- survey
- systematic literature review
- recommender systems
- evaluation
categories: []
date: '2023-10-01'
lastmod: 2023-11-10T19:36:22+01:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2023-11-10T18:35:57.028496Z'
publication_types:
- '2'
abstract: Recommender systems research and practice are fast-developing topics with
  growing adoption in a wide variety of information access scenarios. In this paper,
  we present an overview of research specifically focused on the emphevaluation of
  recommender systems. We perform a systematic literature review, in which we analyze
  57 papers spanning six years (2017–2022). Focusing on the processes surrounding
  evaluation, we dial in on the methods applied, the datasets utilized, and the metrics
  used. Our study shows that the predominant experiment type in research on the evaluation
  of recommender systems is offline experimentation and that online evaluations are
  primarily used in combination with other experimentation methods, e.g., an offline
  experiment. Furthermore, we find that only a few datasets (MovieLens, Amazon review
  dataset) are widely used, while many datasets are used in only a few papers each.
  We observe a similar scenario when analyzing the employed performance metrics—a
  few metrics are widely used (precision, nDCG, and Recall), while many others are
  used in only a few papers. Overall, our review indicates that beyond-accuracy qualities
  are rarely assessed. Our analysis shows that the research community working on evaluation
  has focused on the development of evaluation in a rather narrow scope, with the
  majority of experiments focusing on a few metrics, datasets, and methods.
publication: '*ACM Trans. Recomm. Syst.*'
doi: 10.1145/3629170
links:
- name: URL
  url: https://doi.org/10.1145/3629170
---
