@inproceedings{mohammadi:recsys:2024,
 abstract = {Explainability in recommender systems is both crucial and challenging. Among the state-of-the-art explanation strategies, counterfactual explanation provides intuitive and easily understandable insights into model predictions by illustrating how a small change in the input can lead to a different outcome. Recently, this approach has garnered significant attention, with various studies employing different metrics to evaluate the performance of these explanation methods. In this paper, we investigate the metrics used for evaluating counterfactual explainers for recommender systems. Through extensive experiments, we demonstrate that the performance of recommenders has a direct effect on counterfactual explainers and ignoring it results in inconsistencies in the evaluation results of explainer methods. Our findings highlight an additional challenge in evaluating counterfactual explainer methods and underscore the need to report the recommender performance or consider it in evaluation metrics.},
 address = {New York, NY, USA},
 author = {Mohammadi, Amir Reza and Peintner, Andreas and Müller, Michael and Zangerle, Eva},
 booktitle = {Proceedings of the 18th ACM Conference on Recommender Systems},
 doi = {10.1145/3640457.3691709},
 isbn = {9798400705052},
 keywords = {Counterfactual Explanation, Evaluation, Recommender Systems},
 location = {Bari, Italy},
 numpages = {6},
 pages = {1113–1118},
 publisher = {Association for Computing Machinery},
 series = {RecSys '24},
 title = {Are We Explaining the Same Recommenders? Incorporating Recommender Performance for Evaluating Explainers},
 url = {https://doi.org/10.1145/3640457.3691709},
 year = {2024}
}
