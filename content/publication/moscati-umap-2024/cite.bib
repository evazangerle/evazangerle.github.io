@inproceedings{moscati:umap:2024,
 abstract = {Emotions constitute an important aspect when listening to music. While manual annotations from user studies grounded in psychological research on music and emotions provide a well-defined and fine-grained description of the emotions evoked when listening to a music track, user-generated tags provide an alternative view stemming from large-scale data. In this work, we examine the relationship between these two emotional characterizations of music and analyze their impact on the performance of emotion-based music recommender systems individually and jointly. Our analysis shows that (i) the agreement between the two characterizations, as measured with Cohen’s κ coefficient and Kendall rank correlation, is often low, (ii) Leveraging the emotion profile based on the intensity of evoked emotions from high-quality annotations leads to performances that are stable across different recommendation algorithms; (iii) Simultaneously leveraging the emotion profiles based on high-quality and large-scale annotations allows to provide recommendations that are less exposed to the low accuracy that algorithms might reach when leveraging one type of data, only.},
 address = {New York, NY, USA},
 author = {Moscati, Marta and Strauß, Hannah and Jacobsen, Peer-Ole and Peintner, Andreas and Zangerle, Eva and Zentner, Marcel and Schedl, Markus},
 booktitle = {Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization},
 doi = {10.1145/3627043.3659540},
 isbn = {9798400704338},
 keywords = {Annotation Study, Emotion-based Recommender Systems, Emotions, Music, Music Recommender Systems},
 location = {Cagliari, Italy},
 numpages = {6},
 pages = {159–164},
 publisher = {Association for Computing Machinery},
 series = {UMAP '24},
 title = {Emotion-Based Music Recommendation from Quality Annotations and Large-Scale User-Generated Tags},
 url = {https://doi.org/10.1145/3627043.3659540},
 year = {2024}
}
